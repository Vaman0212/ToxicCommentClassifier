{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e074043",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acd53c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\SHUBHAM\\\\stanfordnlp_resources\\\\hi_hdtb_models\\\\hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\SHUBHAM\\\\stanfordnlp_resources\\\\hi_hdtb_models\\\\hi_hdtb_lemmatizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter, OrderedDict\n",
    "pd.options.mode.chained_assignment = None\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,lemma',lang=\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb17f9",
   "metadata": {},
   "source": [
    "## Loading Vectorizers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a16c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def my_tokenizer(s):\n",
    "    return s.split(' ')\n",
    "tfidf = pickle.load(open('tfidf.pkl','rb'))\n",
    "\n",
    "# Random Forest\n",
    "Rf1 = pickle.load(open('RandomForest_mild.sav','rb'))\n",
    "Rf2 = pickle.load(open('RandomForest_moderate.sav','rb'))\n",
    "Rf3 = pickle.load(open('RandomForest_severe.sav','rb'))\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n",
    "\n",
    "# LSTM\n",
    "model = load_model('LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1219ce",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2234dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating stopwords\n",
    "def gen_stopword():\n",
    "    st=pd.read_csv('hindi_stopwords.txt',sep='\\n')\n",
    "    stopwords=[]\n",
    "    for i in range(len(st)):\n",
    "        stopwords.append(st.loc[i, 'Stopwords'].strip())\n",
    "    return stopwords\n",
    "# lemmatization function\n",
    "def hi_lemma(w):\n",
    "    try:\n",
    "        doc = nlp(w)\n",
    "        tmp = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "        return tmp[0]\n",
    "    except:\n",
    "        return w\n",
    "arr = []\n",
    "def preprocess(text):\n",
    "    # removing url links\n",
    "    func = lambda x: re.sub(r'http\\S+', '', x)\n",
    "    text = func(text)\n",
    "    func = lambda x: re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing new lines and tabs\n",
    "    func = lambda x: re.sub(r\"[\\t\\r]+\", '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing @mention\n",
    "    func = lambda x: re.sub(r'@[\\w]*', '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing all special characters\n",
    "    func = lambda x: re.sub(r\"[`'''`,~,!,@,#,$,%,^,&,*,(,),_,-,+,=,{,[,},},|,\\,:,;,\\\",',<,,,>,.,?,/'''`\\n।]\", '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    func = lambda x: emoji_pattern.sub(r'', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing all remaining characters that aren't hindi devanagari characters or white space\n",
    "    func = lambda x: re.sub(r\"[^ऀ-ॿ\\s]\", '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing stopwords\n",
    "    stopwords = gen_stopword()\n",
    "    func = lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # tokenization\n",
    "    func = lambda x: x.split(' ')\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # lemmatization\n",
    "    func = lambda x: [hi_lemma(y) for y in x]\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # remove repeated tokens\n",
    "    func = lambda x: list(OrderedDict.fromkeys(x))\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # generating clean sentence\n",
    "    sentence = ' '.join(r for r in text)\n",
    "    arr.append(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b8b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "बेवकूफ\n",
      "\n",
      "['बेवकूफ']\n",
      "\n",
      "['बेवकूफ']\n",
      "\n",
      "['बेवकूफ']\n",
      "\n",
      "बेवकूफ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"बेवकूफ\"\"\"\n",
    "print(\"Original Text: \",text,end='\\n\\n')\n",
    "text = [preprocess(text)]\n",
    "for i in arr:\n",
    "    print(i,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe95f75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8387)\t1.0\n"
     ]
    }
   ],
   "source": [
    "text1 = tfidf.transform(text)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c98af750",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mild: 69.55\n",
      "Moderate: 40.62\n",
      "Severe: 0.0\n"
     ]
    }
   ],
   "source": [
    "score = (Rf1.predict_proba(text1)[0][1])*100\n",
    "print(\"Mild:\",round(score,2))\n",
    "score = (Rf2.predict_proba(text1)[0][1])*100\n",
    "print(\"Moderate:\",round(score,2))\n",
    "score = (Rf3.predict_proba(text1)[0][1])*100\n",
    "print(\"Severe:\",round(score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bbd22e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0 1225]]\n"
     ]
    }
   ],
   "source": [
    "text2 = tokenizer.texts_to_sequences(text)\n",
    "text2 = pad_sequences(text2, maxlen=80, padding=\"pre\", truncating=\"pre\")\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e542bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mild: 97.4\n",
      "Moderate: 5.7\n",
      "Severe: 0.8\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(text2)\n",
    "print('Mild:',round(scores[0][0]*100,1))\n",
    "print('Moderate:',round(scores[0][1]*100,1))\n",
    "print('Severe:',round(scores[0][2]*100,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
