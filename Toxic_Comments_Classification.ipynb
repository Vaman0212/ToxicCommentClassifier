{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e074043",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acd53c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\SHUBHAM\\\\stanfordnlp_resources\\\\hi_hdtb_models\\\\hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\SHUBHAM\\\\stanfordnlp_resources\\\\hi_hdtb_models\\\\hi_hdtb_lemmatizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter, OrderedDict\n",
    "pd.options.mode.chained_assignment = None\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,lemma',lang=\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb17f9",
   "metadata": {},
   "source": [
    "## Loading Vectorizers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a16c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def my_tokenizer(s):\n",
    "    return s.split(' ')\n",
    "tfidf = pickle.load(open('tfidf.pkl','rb'))\n",
    "\n",
    "# Count Vectorizer\n",
    "vectorizer = pickle.load(open('../vectorizer.pkl','rb'))\n",
    "\n",
    "# Random Forest\n",
    "Rft = pickle.load(open('../RandomForestT.sav','rb'))\n",
    "Rfc = pickle.load(open('../RandomForestC.sav','rb'))\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n",
    "\n",
    "# LSTM\n",
    "model = load_model('LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1219ce",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2234dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating stopwords\n",
    "def gen_stopword():\n",
    "    st=pd.read_csv('hindi_stopwords.txt',sep='\\n')\n",
    "    stopwords=[]\n",
    "    for i in range(len(st)):\n",
    "        stopwords.append(st.loc[i, 'Stopwords'].strip())\n",
    "    return stopwords\n",
    "# lemmatization function\n",
    "def hi_lemma(w):\n",
    "    try:\n",
    "        doc = nlp(w)\n",
    "        tmp = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "        return tmp[0]\n",
    "    except:\n",
    "        return w\n",
    "arr = []\n",
    "def preprocess(text):\n",
    "    # removing url links\n",
    "    func = lambda x: re.sub(r'http\\S+', '', x)\n",
    "    text = func(text)\n",
    "    func = lambda x: re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing new lines and tabs\n",
    "    func = lambda x: re.sub(r\"[\\t\\r]+\", '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing @mention\n",
    "    func = lambda x: re.sub(r'@[\\w]*', '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing all special characters\n",
    "    func = lambda x: re.sub(r\"[`'''`,~,!,@,#,$,%,^,&,*,(,),_,-,+,=,{,[,},},|,\\,:,;,\\\",',<,,,>,.,?,/'''`\\nред]\", '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    func = lambda x: emoji_pattern.sub(r'', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing all remaining characters that aren't hindi devanagari characters or white space\n",
    "    func = lambda x: re.sub(r\"[^рдА-ре┐\\s]\", '', x)\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # removing stopwords\n",
    "    stopwords = gen_stopword()\n",
    "    func = lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # tokenization\n",
    "    func = lambda x: x.split(' ')\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # lemmatization\n",
    "    func = lambda x: [hi_lemma(y) for y in x]\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # remove repeated tokens\n",
    "    func = lambda x: list(OrderedDict.fromkeys(x))\n",
    "    text = func(text)\n",
    "    arr.append(text)\n",
    "    # generating clean sentence\n",
    "    sentence = ' '.join(r for r in text)\n",
    "    arr.append(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2b8b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  @priya1 рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ #ugly!!!, ЁЯдУЁЯШДЁЯШЖЁЯШВЁЯдгЁЯШВЁЯдгЁЯШВЁЯдг\n",
      "https://t.co/BFRtZXCXrp\n",
      "\n",
      "@priya1 рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ #ugly!!!, ЁЯдУЁЯШДЁЯШЖЁЯШВЁЯдгЁЯШВЁЯдгЁЯШВЁЯдг\n",
      "\n",
      "\n",
      "@priya1 рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ #ugly!!!, ЁЯдУЁЯШДЁЯШЖЁЯШВЁЯдгЁЯШВЁЯдгЁЯШВЁЯдг\n",
      "\n",
      "\n",
      " рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ #ugly!!!, ЁЯдУЁЯШДЁЯШЖЁЯШВЁЯдгЁЯШВЁЯдгЁЯШВЁЯдг\n",
      "\n",
      "\n",
      " рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ ugly ЁЯдУЁЯШДЁЯШЖЁЯШВЁЯдгЁЯШВЁЯдгЁЯШВЁЯдг\n",
      "\n",
      " рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ ugly \n",
      "\n",
      " рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ  \n",
      "\n",
      "рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдмрджреНрд╕реВрд░рдд\n",
      "\n",
      "['рдкреНрд░рд┐рдпрд╛', 'рдмрдбреА', 'рдмреЗрд╡рдХреВрдл', 'рдмрджреНрд╕реВрд░рдд']\n",
      "\n",
      "['рдкреНрд░рд┐рдпрд╛', 'рдмрдбреА', 'рдмреЗрд╡рдХреВрдл', 'рдмрджреНрд╕реВрд░рдд']\n",
      "\n",
      "['рдкреНрд░рд┐рдпрд╛', 'рдмрдбреА', 'рдмреЗрд╡рдХреВрдл', 'рдмрджреНрд╕реВрд░рдд']\n",
      "\n",
      "рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдмрджреНрд╕реВрд░рдд\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"@priya1 рдкреНрд░рд┐рдпрд╛ рдмрдбреА рдмреЗрд╡рдХреВрдл рдФрд░ рдмрджреНрд╕реВрд░рдд рд╣реИ #ugly!!!, ЁЯдУЁЯШДЁЯШЖЁЯШВЁЯдгЁЯШВЁЯдгЁЯШВЁЯдг\n",
    "https://t.co/BFRtZXCXrp\"\"\"\n",
    "print(\"Original Text: \",text,end='\\n\\n')\n",
    "text = [preprocess(text)]\n",
    "for i in arr:\n",
    "    print(i,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afe95f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8387)\t1.0\n"
     ]
    }
   ],
   "source": [
    "text1 = tfidf.transform(text)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c98af750",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.337, 0.663]]), array([[0.793, 0.207]]), array([[1., 0.]])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rft.predict_proba(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8ee0285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8387)\t1\n"
     ]
    }
   ],
   "source": [
    "text2 = vectorizer.transform(text)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d1260bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.266, 0.734]]), array([[0.864, 0.136]]), array([[1., 0.]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rfc.predict_proba(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bbd22e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0 1225]]\n"
     ]
    }
   ],
   "source": [
    "text3 = tokenizer.texts_to_sequences(text)\n",
    "text3 = pad_sequences(text3, maxlen=80, padding=\"pre\", truncating=\"pre\")\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e542bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mild: 97.4\n",
      "Moderate: 5.7\n",
      "Severe: 0.8\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(text3)\n",
    "print('Mild:',round(scores[0][0]*100,1))\n",
    "print('Moderate:',round(scores[0][1]*100,1))\n",
    "print('Severe:',round(scores[0][2]*100,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
